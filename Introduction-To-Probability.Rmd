---
title: "Introduction to Probability"
author: "Illarion Jabine"
date: "14/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### 1. Required packages
 * stat: core R package


### 2. Key terms
 * Probability Distributions 
 * Discreet Probability Distribution
 * Continues Probability Distribution

### 3. Useful Links & books
 $ Camm et al: Business Analytics, 3rd edition
 $ Anderson et al: Statistics for Business and Economics
 $ The CRAN task view on distributions: https://CRAN.R-project.org/view=Distributions
 
### 4. Introduction

Every business needs to deal with uncertainties of different kinds. We can have data to provide information on possible outcomes for business decisions.
However, exact outcome is never known because of many parameters that are out of control or unknown.
Probability is the numerical measure of the likelihood that an event will occur. 
Probability can serve as a measure of the uncertainty associated with an event (Camm et al, 2019).
There are several basic concepts in the probability theory. Outcomes and events are the foundation in probability study.
 1. A random experiment can be defines as a process that generates well-defined outcomes.
 For example, tossing a coin is an experiment with possible outcomes - tail or head.
 
 2. Sample space for a random experiment is all possible outcomes. For coin toss the sample space: S = {Head, Tail}
 
 3. Event can be defined as a collection of outcomes for the random experiment. 
The definition of an event depends on a research question asked by a decision maker.
For example, from historical data a manager knows that monthly revenues can be 5, 6, 9, 12 and 15 billion dollars. 
The manager can be intersted in the probability of an event when the revenue is more than 9 billion. The event A = {9,12,15}.
Event A is said to occur if any one of these outcomes occur, which is either 9 OR (plus) 12 OR (plus) 15.
By constracting probability frequency table we can calculate the probability of each outcome.
It is important tto remember that The probability of an event is equal to the sum of the probabilities of outcomes for the event.
P(A) = P(9) + P(12) + P(15)

 4. Complement of event A is defined to be the event consisting of all outcomes that are not in A. For example, if event A = {9,12,15} then A' (or complement) = {5,6}.
 Either event A or its complement A' must occur, which gives us this:
 P(A) + P(A') = 1
 
 5. Addition law - is useful when we want to know the probability that at least one of two events will occur.
That means we want to know the probability that event A or event B occurs or both events occur. Important to note that we use here OR.
Closely related to addition law is two concepts of the union and the intersection of events.
 * The union of A and B is defined as the event containing all outcomes belonging to A
or B or both. The union of A and B is written as by A U B.
 * The intersection of A and B is the event containing the outcomes that belong to both A and B. The intersection of A and B is defined as U upside down. Here I will depict it as IN.
 The addition law is used to compute the probability of the union of two events, i.e. the probability that event A or event B occurs or both events occur.
 P(A U B) =  P(A) + P(B) - P(IN(A,B))

 6. Mutually exclusive events are a special case of addition law. 
Events A and B are mutually exclusive if the occurrence of one event precludes the occurrence of the other. The mutually exclusive evenats have their intersection with no sample points or no outcomes in common. That is P(IN(A,B)) = 0.
The addition law for mutually exclusive events:
P(A U B) = P(A) + P(B)
Note: Two events with nonzero probabilities cannot be both mutually exclusive and independent!

 7. Conditional probability
Sometimes the probability of one event is dependent on if another related event has already occured. So, by knowing that even B has happened gives us some information to calculate the new probability of a related event A. This new probability of event A is called a conditional probability and is written P(A|B). | indicates that we are considering the probability of event A given the condition that event B has occurred.
Example of a conditional probability can be as follows, customer churning, i.e. to calculate the probability a customer will leave if a customer has a high education or not.
Conditional probability is calculated using the following formula:
P(A | B) = P(IN(A,B))/P(B)

 8. Independent events
If the probability of event A is not changed by the existence of event B, 
i.e. P(A | B) = P(A), or P(B | A) = P(B) then events A and B are independent events.

 9. The multiplication law can be used to calculate the probability of the intersection of two events, and it is based on the definition of conditional probability:
 P(IN(A,B)) = P(B)*P(A | B) or P(IN(A,B)) = P(A)*P(B | A)
 
Multiplication Law for Independent Events:  P(IN(A,B))  = P(A)P(B)

 10. Random variable
A random variable is a numerical description of the outcome of an experiment. It associates a numerical value with each possible experimental outcome. A random variable can be either discrete or continuous depending on the numerical values it assumes.
 * A discrete random variable can take either a finite or an infinite but countable number of values, i.e. 0,1,2,3,...
 * A continuous random variable can take any value in an interval, the set of values it can take is not countabl and its cumulative distribution function (CDF) can be obtained by integrating a probability density function (PDF).

 11. Probability distribution
The probability distribution for a random variable describes how probabilities are distributed over the values of the random variable. For a discrete random variable x, a probability function f(x), provides the probability for each value of the random variable.

 12. The expected value of a random variable is a weighted average of the values of the random variable where the weights are the probabilities. It is a measure of the central location for the
random variable.
E(x) = μ = sum(x*f(x)), where f(x) is a probability function.

 13. The variance is a weighted average of the squared deviations of a random variable from its mean. The weights are the probabilities.
 Var(x) = σ2 = sum((x - μ)^2f(x). Deviation x - μ, measures how far a particular value of the random variable is from the expected value. In computing the variance of a random variable, the deviations are squared and then weighted by the corresponding value of the probability function.

### 5. Probability Distribution R Functions

For most of the classical distributions, base R provides probability distribution functions (p), density functions (d), quantile functions (q), and random number generation (r).
p - probability distribution function
q - quantile, the inverse DF for a continuous random variable or the quantile function
d - density, the probability mass function (PMF) for a discrete random variable or the probability density function (PDF) for a continuous random variable
r - a random variable having the specified distribution
 * For a continuous distribution use p and q functions
 * For a discrete distribution use d function that calculates the density 
    Distribution                   p         q         d         r        
    <chr>                          <chr>     <chr>     <chr>     <chr>    
  1 Beta                           pbeta     qbeta     dbeta     rbeta    
  2 Binomial                       pbinom    qbinom    dbinom    rbinom   
  3 Cauchy                         pcauchy   qcauchy   dcauchy   rcauchy  
  4 Chi-Square                     pchisq    qchisq    dchisq    rchisq   
  5 Exponential                    pexp      qexp      dexp      rexp     
  6 F                              pf        qf        df        rf       
  7 Gamma                          pgamma    qgamma    dgamma    rgamma   
  8 Geometric                      pgeom     qgeom     dgeom     rgeom    
  9 Hypergeometric                 phyper    qhyper    dhyper    rhyper   
 10 Logistic                       plogis    qlogis    dlogis    rlogis   
 11 Log Normal                     plnorm    qlnorm    dlnorm    rlnorm   
 12 Negative Binomial              pnbinom   qnbinom   dnbinom   rnbinom  
 13 Normal                         pnorm     qnorm     dnorm     rnorm    
 14 Poisson                        ppois     qpois     dpois     rpois    
 15 Student t                      pt        qt        dt        rt       
 16 Studentized Range              ptukey    qtukey    dtukey    rtukey   
 17 Uniform                        punif     qunif     dunif     runif    
 18 Weibull                        pweibull  qweibull  dweibull  rweibull 
 19 Wilcoxon Rank Sum Statistic    pwilcox   qwilcox   dwilcox   rwilcox  
 20 Wilcoxon Signed Rank Statistic psignrank qsignrank dsignrank rsignrank