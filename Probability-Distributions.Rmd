---
title: "Probability Distributions"
author: "Illarion Jabine"
date: "8/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### 1. Required packages
 * stat: core R package


### 2. Key terms
 * Probability Distributions 
 * Discreet Probability Distribution
 * Continues Probability Distribution

### 3. Useful Links & books
 $ Camm et al: Business Analytics, 3rd edition
 $ Anderson et al: Statistics for Business and Economics
 
### 4. Introduction

Every business needs to deal with uncertainties of different kinds. We can have data to provide information on possible outcomes for business decisions.
However, exact outcome is never known because of many parameters that are out of control or unknown.
Probability is the numerical measure of the likelihood that an event will occur. 
Probability can serve as a measure of the uncertainty associated with an event (Camm et al, 2019).
There are several basic concepts in the probability theory. Outcomes and events are the foundation in probability study.
 1. A random experiment can be defines as a process that generates well-defined outcomes.
 For example, tossing a coin is an experiment with possible outcomes - tail or head.
 
 2. Sample space for a random experiment is all possible outcomes. For coin toss the sample space: S = {Head, Tail}
 
 3. Event can be defined as a collection of outcomes for the random experiment. 
The definition of an event depends on a research question asked by a decision maker.
For example, from historical data a manager knows that monthly revenues can be 5, 6, 9, 12 and 15 billion dollars. 
The manager can be intersted in the probability of an event when the revenue is more than 9 billion. The event A = {9,12,15}.
Event A is said to occur if any one of these outcomes occur, which is either 9 OR (plus) 12 OR (plus) 15.
By constracting probability frequency table we can calculate the probability of each outcome.
It is important tto remember that The probability of an event is equal to the sum of the probabilities of outcomes for the event.
P(A) = P(9) + P(12) + P(15)

 4. Complement of event A is defined to be the event consisting of all outcomes that are not in A. For example, if event A = {9,12,15} then A' (or complement) = {5,6}.
 Either event A or its complement A' must occur, which gives us this:
 P(A) + P(A') = 1
 
 5. Addition law - is useful when we want to know the probability that at least one of two events will occur.
That means we want to know the probability that event A or event B occurs or both events occur. Important to note that we use here OR.
Closely related to addition law is two concepts of the union and the intersection of events.
 * The union of A and B is defined as the event containing all outcomes belonging to A
or B or both. The union of A and B is written as by A U B.
 * The intersection of A and B is the event containing the outcomes that belong to both A and B. The intersection of A and B is defined as U upside down. Here I will depict it as IN.
 The addition law is used to compute the probability of the union of two events, i.e. the probability that event A or event B occurs or both events occur.
 P(A U B) =  P(A) + P(B) - P(IN(A,B))

 6. Mutually exclusive events are a special case of addition law. 
Events A and B are mutually exclusive if the occurrence of one event precludes the occurrence of the other. The mutually exclusive evenats have their intersection with no sample points or no outcomes in common. That is P(IN(A,B)) = 0.
The addition law for mutually exclusive events:
P(A U B) = P(A) + P(B)
Note: Two events with nonzero probabilities cannot be both mutually exclusive and independent!

 7. Conditional probability
Sometimes the probability of one event is dependent on if another related event has already occured. So, by knowing that even B has happened gives us some information to calculate the new probability of a related event A. This new probability of event A is called a conditional probability and is written P(A|B). | indicates that we are considering the probability of event A given the condition that event B has occurred.
Example of a conditional probability can be as follows, customer churning, i.e. to calculate the probability a customer will leave if a customer has a high education or not.
Conditional probability is calculated using the following formula:
P(A | B) = P(IN(A,B))/P(B)

 8. Independent events
If the probability of event A is not changed by the existence of event B, 
i.e. P(A | B) = P(A), or P(B | A) = P(B) then events A and B are independent events.

 9. The multiplication law can be used to calculate the probability of the intersection of two events, and it is based on the definition of conditional probability:
 P(IN(A,B)) = P(B)*P(A | B) or P(IN(A,B)) = P(A)*P(B | A)
 
Multiplication Law for Independent Events:  P(IN(A,B))  = P(A)P(B)

 10. Random variable
A random variable is a numerical description of the outcome of an experiment. It associates a numerical value with each possible experimental outcome. A random variable can be either discrete or continuous depending on the numerical values it assumes.
 * A discrete random variable can take either a finite or an infinite but countable number of values, i.e. 0,1,2,3,...
 * A continuous random variable can take any value in an interval, the set of values it can take is not countabl and its cumulative distribution function (CDF) can be obtained by integrating a probability density function (PDF).

 11. Probability distribution
The probability distribution for a random variable describes how probabilities are distributed over the values of the random variable. For a discrete random variable x, a probability function f(x), provides the probability for each value of the random variable.

 12. The expected value of a random variable is a weighted average of the values of the random variable where the weights are the probabilities. It is a measure of the central location for the
random variable.
E(x) = μ = sum(x*f(x)), where f(x) is a probability function.

 13. The variance is a weighted average of the squared deviations of a random variable from its mean. The weights are the probabilities.
 Var(x) = σ2 = sum((x - μ)^2f(x). Deviation x - μ, measures how far a particular value of the random variable is from the expected value. In computing the variance of a random variable, the deviations are squared and then weighted by the corresponding value of the probability function.

### 5. Probability Distribution R Functions

p - probability distribution function
q - quantile, the inverse DF for a continuous random variable or the quantile function
d - density, the probability mass function (PMF) for a discrete random variable or the probability density function (PDF) for a continuous random variable
r - a random variable having the specified distribution
 * For a continuous distribution use p and q functions
 * For a discrete distribution use d function that calculates the density 
    Distribution                   p         q         d         r        
    <chr>                          <chr>     <chr>     <chr>     <chr>    
  1 Beta                           pbeta     qbeta     dbeta     rbeta    
  2 Binomial                       pbinom    qbinom    dbinom    rbinom   
  3 Cauchy                         pcauchy   qcauchy   dcauchy   rcauchy  
  4 Chi-Square                     pchisq    qchisq    dchisq    rchisq   
  5 Exponential                    pexp      qexp      dexp      rexp     
  6 F                              pf        qf        df        rf       
  7 Gamma                          pgamma    qgamma    dgamma    rgamma   
  8 Geometric                      pgeom     qgeom     dgeom     rgeom    
  9 Hypergeometric                 phyper    qhyper    dhyper    rhyper   
 10 Logistic                       plogis    qlogis    dlogis    rlogis   
 11 Log Normal                     plnorm    qlnorm    dlnorm    rlnorm   
 12 Negative Binomial              pnbinom   qnbinom   dnbinom   rnbinom  
 13 Normal                         pnorm     qnorm     dnorm     rnorm    
 14 Poisson                        ppois     qpois     dpois     rpois    
 15 Student t                      pt        qt        dt        rt       
 16 Studentized Range              ptukey    qtukey    dtukey    rtukey   
 17 Uniform                        punif     qunif     dunif     runif    
 18 Weibull                        pweibull  qweibull  dweibull  rweibull 
 19 Wilcoxon Rank Sum Statistic    pwilcox   qwilcox   dwilcox   rwilcox  
 20 Wilcoxon Signed Rank Statistic psignrank qsignrank dsignrank rsignrank

### 6. Discrete Probability Distributions
 
 All major text books always give these three disctributions as examples: 
  1. Binomial
  2. Poisson
  3. Hypergeometric
  
 6.1. Binomial Probability Distribution
 
 It is associated with a multiple-step experiment also know as the binomial experiment This experiment has two possible outcomes: either success or failure.
 * Propoerties of a binomial experiment:
   1. The experiment consists of a sequence of n identical trials.
   2. Two outcomes are possible on each trial: one outcome is refered as a success and the other outcome as a failure.
   3. The probability of a success, denoted by p, does not change from trial to trial. Consequently, the probability of a failure, denoted by 1 - p, does not change from trial to trial.
   4. The trials are independent.
In a binomial experiment, our interest is in the number of successes occurring in the n trials. If we let x denote the number of successes occurring in the n trials, we see that x can assume the values of 0, 1, 2, 3, . . . , n.
 * Business examples where binomial distribution can be used:
   -> What is the probability if out of 100 customers 20 will buy, knowing from historical figures (on the basis of past experience) that the  probability that a randomly selected customer will make a purchase is .20.
   -> What is probability that out of 50 randomly selected products, 10 are faulty.
We try to calculate the probability of x successes in the n trials.

f(x) = CNX*p^x(1 - p)^(n-x)
Where:
x = the number of successes
p = the probability of a success on one trial
n = the number of trials
f(x) = the probability of x successes in n trials
CNX = n!/x!(n - x)!
In R to calculate binomial probability use dbinom() function:

```{r binomial probability calculation}
# what is probablity that 4 out of 10 next customers will make a purchase? The probablity of a purchase on one trial is .3
dbinom(x = 4,size = 10,prob = 0.3)

# Let's display the probability distribution function for different values of x.
plot(dbinom(x = 1:100,size = 10,prob = 0.3),type = "h",col = 3, lwd = 3, ylab = "Probability")

```

Expected value: E(x) = np
Variance: Var(x) = np(1-p),
where n - known number of trials and p -  known probability of success


 6.2. Poisson Probability Distribution

Poisson distribution is useful in estimating the number of occurrences over a specified interval of time or space. For example, number of arrivals at a bar in one hour, the number of repairs per 20 kilometers of a highway, etc.
The following two properties must be satisfied for a random variable to follow Poisson probability distribution:
 1. The probability of an occurrence is the same for any two intervals of equal length.
 2. The occurrence or nonoccurrence in any interval is independent of the occurrence or nonoccurrence in any other interval.
POISSON PROBABILITY FUNCTION:
f(x)=(μ^x*e^-μ)/x!
Where:

f(x)- the probability of x occurrences in an interval
x - a discrete random variable indicating the number of occurrences in the interval
μ - expected value or mean number of occurrences
e - 2.71828
Let say we want to know the probability of x = 10 customers arriving in a shop in 1 hour. 
Analysis of historical data shows that the average number of customers arriving in a 60-minute period of time is 15 (lambda).
```{r poisson probability calculation }
prob_10_customers <- dpois(10,15)

plot(dpois(1:100,15),type = "h",col = 3, lwd = 3, ylab = "Probability")
```
Note: mean and variance of poisson distribution are equal.

 6.3. Hypergeometric Probability Distribution
 
The hypergeometric probability distribution is related to the binomial distribution, however they differ in two ways:

In the hypergeometric distribution:
 * the trials are not independent. It's because a sample of size n is randomly selected without replacement from a population of N items.
 * the probability of success changes from trial to trial, because we take a sample without replacement.
Notation:
N: The number of items in the population.
k: The number of items in the population that are classified as successes.
n: The number of items in the sample.
x: The number of items in the sample that are classified as successes.
In the population, k items can be classified as successes, and N - k items can be classified as failures.
If it was a binomial experiment (with replacement), then the probability of success be constant on every trial, whereas here it changes on every trial (without replacement).
Suppose a population consists of N items, k of which are successes. And a random sample drawn from that population consists of n items, x of which are successes. The hypergeometric probability is calculated using this formula:
f(x; N, n, k) = [ kCx ] [ N-kCn-x ] / [ NCn ]

[C] - combinations

Business example from Anderson et al: Statistics for Business and Economics book:

Axline Computers manufactures personal computers at two plants, one in Texas and the other in Hawaii. The Texas plant has 40 employees; the Hawaii plant has 20. A random sample of 10 employees is to be asked to fill out a benefits questionnaire.

 1. What is the probability that one of the employees in the sample works at the plant in
Hawaii?
 2. What is the probability that two or more of the employees in the sample work at the
plant in Hawaii?

In this example we have the following information:
N = Population Size = 40 + 20 = 60
n = Sample size = 10
k = Number of observed success = number of workers in Hawaii = 20.
```{r hypergeometric distribution 1}
# 1. What is the probability that one of the employees in the sample works at the plant in Hawaii?
dhyper(x = 1,k = 10,m = 20,n = 40)

# k = Sample size = 10
# m = Number of observed success = number of workers in Hawaii = 20
# n = Number of failures (employees working in Texas) = 40

```
```{r hypergeometric distribution 2}
# 2. What is the probability that two or more of the employees in the sample work at the plant in Hawaii?
# 1 - f(0) - f(1)
p_x_2 <- 1 - dhyper(x = 0,k = 10,m = 20,n = 40) - dhyper(x = 1,k = 10,m = 20,n = 40)


```

